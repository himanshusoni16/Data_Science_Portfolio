{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-1\n",
    "\n",
    "### 1. Scrape the details of most viewed videos on YouTube from Wikipedia:\n",
    "\n",
    "* Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/ You need to find following details:\n",
    "* A) Rank\n",
    "* B) Name\n",
    "* C) Artist\n",
    "* D) Upload date\n",
    "* E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the basic Libraries import requests\n",
    "\n",
    "import time\n",
    "import selenium\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "## lets connect with web driver \n",
    "\n",
    "#driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "## spacifiying the url\n",
    "URL = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "## scrap the tabuler data of the top 30 most view videos\n",
    "Top_View_table=tables[0]\n",
    "Top_View_data = [[cell.text for cell in row.find_all([\"th\",\"td\"])]\n",
    "                               for row in Top_View_table.find_all(\"tr\")]\n",
    "Top_View_df = pd.DataFrame(Top_View_data)\n",
    "#To move the first row to the headers, simply type\n",
    "Top_View_df.columns = Top_View_df.iloc[0,:]\n",
    "Top_View_df.drop(index=0,inplace=True)\n",
    "Top_View_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### removed the \"\\n\" in the cloumns name \n",
    "columns=[]\n",
    "for i in Top_View_df.columns:\n",
    "    columns.append(i.replace(\"\\n\",\"\"))\n",
    "Top_View_df.columns = columns\n",
    "### replace the first column name \n",
    "Top_View_df.rename(columns={\"No.\":\"Ranks\"},inplace=True)\n",
    "## Drop the last coulmn in the data set \n",
    "Top_View_df.drop('Note', inplace=True, axis=1)\n",
    "Top_View_df.head() ## print the top 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-2\n",
    "\n",
    "### 2-Scrape the details team India’s international fixtures from bcci.tv. Url = https://www.bcci.tv/.\n",
    "### You need to find following details:\n",
    "* A) Match title (I.e. 1st ODI)\n",
    "* B) Series\n",
    "* C) Place\n",
    "* D) Date\n",
    "* E) Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the basic Libraries import requests\n",
    "\n",
    "import time\n",
    "import selenium\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "\n",
    "## spacifiying the url\n",
    "\n",
    "Url= \"https://www.bcci.tv/international/fixtures\"\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "\n",
    "### creating data frame \n",
    "\n",
    "Match_title = []\n",
    "Series = []\n",
    "Place = []\n",
    "Date = []\n",
    "Time = []\n",
    "Month =[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Match_title_tag = driver.find_elements_by_xpath('//strong[@class=\"fixture__name fixture__name--with-margin\"]') ## for Smartphone_name\n",
    "for i in Match_title_tag:\n",
    "    try :#exception handling for nosuchelementexception\n",
    "        Match_title.append(i.text) \n",
    "    except NoSuchElementException   as e:\n",
    "        Match_title.append(\"--\")\n",
    "\n",
    "Match_title[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Series_tag = driver.find_elements_by_xpath('//span[@class=\"u-unskewed-text fixture__tournament-label u-truncated\"]') ## for Smartphone_name\n",
    "for i in Series_tag:\n",
    "    try :#exception handling for nosuchelementexception\n",
    "        Series.append(i.text) \n",
    "    except NoSuchElementException   as e:\n",
    "        Series.append(\"--\")\n",
    "\n",
    "Series[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Place_tag = driver.find_elements_by_xpath('//p[@class=\"fixture__additional-info\"]/span') ## for Smartphone_name\n",
    "for i in Place_tag:\n",
    "    try :#exception handling for nosuchelementexception\n",
    "        Place.append(i.text) \n",
    "    except NoSuchElementException   as e:\n",
    "        Place.append(\"--\")\n",
    "\n",
    "Place[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Date_tag = driver.find_elements_by_xpath('//span[@class=\"fixture__date\"]') ## for Smartphone_name\n",
    "for i in Date_tag:\n",
    "    try :#exception handling for nosuchelementexception\n",
    "        Date.append(i.text) \n",
    "    except NoSuchElementException   as e:\n",
    "        Date.append(\"--\")\n",
    "\n",
    "Date[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Month_tag = driver.find_elements_by_xpath('//span[@class=\"fixture__month\"]') ## for Smartphone_name\n",
    "for i in Month_tag:\n",
    "    try :#exception handling for nosuchelementexception\n",
    "        Month.append(i.text) \n",
    "    except NoSuchElementException   as e:\n",
    "        Month.append(\"--\")\n",
    "\n",
    "Month[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_tag = driver.find_elements_by_xpath('//span[@class=\"fixture__time\"]') ## for Smartphone_name\n",
    "for i in Time_tag:\n",
    "    try :#exception handling for nosuchelementexception\n",
    "        Time.append(i.text) \n",
    "    except NoSuchElementException   as e:\n",
    "        Time.append(\"--\")\n",
    "\n",
    "Time[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the Data fame \n",
    "\n",
    "BCCI_df = pd.DataFrame({})\n",
    "BCCI_df[\"Match_title\"]= Match_title\n",
    "BCCI_df[\"Series\"]= Series\n",
    "BCCI_df[\"Place\"] = Place\n",
    "BCCI_df[\"Date\"] =  Date\n",
    "BCCI_df[\"Month\"] = Month\n",
    "BCCI_df[\"Time\"] = Time\n",
    "\n",
    "BCCI_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-3\n",
    "\n",
    "### Scrape the details of selenium exception from guru99.com. Url = https://www.guru99.com/\n",
    "### You need to find following details:\n",
    "* A) Name\n",
    "* B) Description\n",
    "### Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the basic Libraries import requests\n",
    "\n",
    "import time\n",
    "import selenium\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "## spacifiying the url\n",
    "Url= \"https://www.guru99.com/\"\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "time.sleep(4)\n",
    "### click in the selenium button \n",
    "driver.find_element_by_xpath('//*[@title=\"Selenium\"]').click()\n",
    "time.sleep(4)\n",
    "driver.find_element_by_xpath('//*[@id=\"g-mainbar\"]/div/div/div/div/div/div/div[2]/table[5]/tbody/tr[34]/td[1]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_string = driver.current_url\n",
    "print(\"URL Extracted: \", url_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the BeautifulSoup for extect the Table\n",
    "\n",
    "page = requests.get(url_string)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "## scrap the tabuler data and find following details A) Name B) Description\n",
    "Common_Exceptions_table=tables[0]\n",
    "Common_Exceptions_table_data = [[cell.text for cell in row.find_all([\"th\",\"td\"])]\n",
    "                               for row in Common_Exceptions_table.find_all(\"tr\")]\n",
    "\n",
    "Common_Exceptions_table_data_df = pd.DataFrame(Common_Exceptions_table_data)\n",
    "#To move the first row to the headers, simply type\n",
    "Common_Exceptions_table_data_df.columns = Common_Exceptions_table_data_df.iloc[0,:]\n",
    "Common_Exceptions_table_data_df.drop(index=0,inplace=True)\n",
    "Common_Exceptions_table_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question:-4\n",
    "### Scrape the details of State-wise GDP of India from statisticstime.com. Url = http://statisticstimes.com/\n",
    "### You have to find following details:\n",
    "* A) Rank\n",
    "* B) State\n",
    "* C) GSDP(18-19)\n",
    "* D) GSDP(17-18)\n",
    "* E) Share(2017)\n",
    "* F) GDP($ billion)\n",
    "## Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the basic Libraries import requests\n",
    "\n",
    "import time\n",
    "import selenium\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "## spacifiying the url\n",
    "Url= \"http://statisticstimes.com/\"\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "time.sleep(4)\n",
    "## Click on the economy page\n",
    "## Click on the economy page\n",
    "URL=driver.find_element_by_xpath('//*[@id=\"top\"]/div[2]/div[2]/div/a[3]').get_attribute(\"href\")\n",
    "driver.get(URL)\n",
    "time.sleep(3)\n",
    "driver.get(driver.find_element_by_xpath('/html/body/div[2]/div[2]/div[2]/ul/li[1]/a').get_attribute(\"href\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_URL = driver.current_url\n",
    "print(\"Final_URL\",final_URL)\n",
    "## Using the BeautifulSoup for extect the Table\n",
    "\n",
    "page = requests.get(final_URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "GDP_tables = soup.find_all(\"table\")\n",
    "\n",
    "## scrap the tabuler data and find following details A) Name B) Description\n",
    "GDPS_table=GDP_tables[1]\n",
    "GDPS_table_data = [[cell.text for cell in row.find_all([\"th\",\"td\"])]\n",
    "                               for row in GDPS_table.find_all(\"tr\")]\n",
    "\n",
    "## creating the dataframe \n",
    "GDPS_table_data_df = pd.DataFrame(GDPS_table_data,columns=[\"Rank\",\"State\",\"GSDP(CrINR_current prices)(19-20)\", \"GSDP(CrINR_current prices)(18-19)\",\"Share\",\"GDP($billion)\",\"GSDP (Cr INR at 2011-12 prices)(19-20)\",\"GSDP (Cr INR at 2011-12 prices)(18-19)\"])\n",
    "GDPS_table_data_df.drop([0, 1],inplace=True)\n",
    "GDPS_table_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDPS_table_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-5\n",
    "\n",
    "### Scrape the details of trending repositories on Github.com. Url = https://github.com/\n",
    "### You have to find the following details:\n",
    "* A) Repository title\n",
    "* B) Repository description\n",
    "* C) Contributors count\n",
    "* D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the basic Libraries import requests\n",
    "\n",
    "import time\n",
    "import selenium\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "## spacifiying the url\n",
    "Url= \" https://github.com/\"\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tranding = driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/div/ul[2]/li[3]/a\")\n",
    "URl = Tranding.get_attribute(\"href\")\n",
    "driver.get(URl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "## creating Empty list\n",
    "Repository_title = []\n",
    "Repository_description = []\n",
    "Contributors_count = []\n",
    "Language_used = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repository_title\n",
    "\n",
    "Repository_title_tag = driver.find_elements_by_xpath('//h1[@class=\"h3 lh-condensed\"]')\n",
    "for i in Repository_title_tag:\n",
    "    Repository_title.append(i.text)\n",
    "Repository_title[0:5]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Repository_description\n",
    "\n",
    "Repository_description_tag = driver.find_elements_by_xpath('//p[@class=\"col-9 text-gray my-1 pr-4\"]')\n",
    "for i in Repository_description_tag:\n",
    "    Repository_description.append(i.text)\n",
    "Repository_description[0:5]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Repository_description\n",
    "\n",
    "Language_used_tag = driver.find_elements_by_xpath('//span[@itemprop=\"programmingLanguage\"]')\n",
    "for i in Language_used_tag:\n",
    "    Language_used.append(i.text)\n",
    "Language_used[0:5]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Contributors_count\n",
    "\n",
    "Contributors_count_tag = driver.find_elements_by_xpath('//a[@class=\"muted-link d-inline-block mr-3\"]')\n",
    "for i in Contributors_count_tag:\n",
    "    Contributors_count.append(i.text)\n",
    "Contributors_count[0:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_df = pd.DataFrame({})\n",
    "git_df[\"Repository_title\"] = Repository_title[0:20]\n",
    "git_df[\"Repository_description\"] = Repository_description[0:20]\n",
    "git_df[\"Language_used\"] = Language_used[0:20]\n",
    "git_df[\"Contributors_count\"] = Contributors_count[0:20]\n",
    "git_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-6\n",
    "\n",
    "### Scrape the details of top 100 songs on billiboard.com. Url = https://www.billiboard.com/\n",
    "### You have to find the following details:\n",
    "* A) Song name\n",
    "* B) Artist name\n",
    "* C) Last week rank\n",
    "* D) Peak rank\n",
    "* E) Weeks on board\n",
    "### Note: - From the home page you have to click on the charts option then hot 100-page link through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the basic Libraries import requests\n",
    "\n",
    "import time\n",
    "import selenium\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "## spacifiying the url\n",
    "Url=  \"https://www.billboard.com/\"\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOT_100 = driver.find_element_by_xpath('//ul[@class=\"header__subnav__list display--flex flex--left-center flex-md--center-center\"]/li[3]/a')\n",
    "driver.get(HOT_100.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Song_Titel = []\n",
    "Artist_Band_Name = []\n",
    "Last_week_rank= []\n",
    "Peak_rank= []\n",
    "Weeks_on_board= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extrect the Song_Titel \n",
    "Song_Titel_tag = driver.find_elements_by_xpath(\"//span[@class='chart-element__information__song text--truncate color--primary']\")\n",
    "for i in Song_Titel_tag:\n",
    "    Song_Titel.append(i.text)\n",
    "    \n",
    "Song_Titel[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extrect the Artist_Band_Name \n",
    "Artist_Band_Name_tag = driver.find_elements_by_xpath(\"//span[@class='chart-element__information__artist text--truncate color--secondary']\")\n",
    "for i in Artist_Band_Name_tag:\n",
    "    Artist_Band_Name.append(i.text)\n",
    "    \n",
    "Artist_Band_Name[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extrect the Last_week_rank\n",
    "Last_week_rank_tag = driver.find_elements_by_xpath(\"//span[@class='chart-element__meta text--center color--secondary text--last']\")\n",
    "for i in Last_week_rank_tag:\n",
    "    Last_week_rank.append(i.text)\n",
    "    \n",
    "Last_week_rank[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extrect the Last_week_rank\n",
    "Peak_rank_tag = driver.find_elements_by_xpath(\"//span[@class='chart-element__meta text--center color--secondary text--peak']\")\n",
    "for i in Peak_rank_tag:\n",
    "    Peak_rank.append(i.text)\n",
    "    \n",
    "Peak_rank[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extrect the Last_week_rank\n",
    "Weeks_on_board_tag = driver.find_elements_by_xpath(\"//span[@class='chart-element__meta text--center color--secondary text--week']\")\n",
    "for i in Weeks_on_board_tag:\n",
    "    Weeks_on_board.append(i.text)\n",
    "    \n",
    "Weeks_on_board[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating dataframe\n",
    "Top_100_songs_df = pd.DataFrame({})\n",
    "Top_100_songs_df[\"Song_Titel\"] = Song_Titel\n",
    "Top_100_songs_df[\"Artist_Band_Name\"] = Artist_Band_Name\n",
    "Top_100_songs_df[\"Last_week_rank\"] = Last_week_rank\n",
    "Top_100_songs_df[\"Peak_rank\"] = Peak_rank\n",
    "Top_100_songs_df[\"Weeks_on_board\"] = Weeks_on_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_100_songs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-7\n",
    "\n",
    "### Scrape the details of Data science recruiters from naukri.com. Url = https://www.naukri.com/\n",
    "### You have to find the following details:\n",
    "* A) Name\n",
    "* B) Designation\n",
    "* C) Company\n",
    "* D) Skills they hire for\n",
    "* E) Location\n",
    "### Note:- From naukri.com homepage click on the recruiters option and the on the search pane type Data science and click on \n",
    "### search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "\n",
    "## spacifiying the url\n",
    "Url=\"https://www.naukri.com/\"\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "\n",
    "## check the All input box ir the page\n",
    "inputboxs = driver.find_elements(By.CLASS_NAME,\"sugInp\")\n",
    "print(len(inputboxs))\n",
    "\n",
    "## Enter “Data Scientist” in “Skill,Designations,Companies” field \n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section/div/form/div[1]/div/div/div/div[1]/div[2]/input\").send_keys(\"Data Scientist\")\n",
    "\n",
    "# now click the search button.\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section/div/form/div[3]/button\").click()\n",
    "time.sleep(3)\n",
    "\n",
    "    \n",
    "## create the 4 empty lists\n",
    "\n",
    "jobs_title=[]\n",
    "companies_name=[]\n",
    "job_location=[]\n",
    "expriences=[]\n",
    "Skills = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    jobs_tag=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]') # for join titles\n",
    "    for i in jobs_tag:\n",
    "        try:\n",
    "            jobs_title.append(i.text)\n",
    "        except NoSuchElementException   as e:\n",
    "            jobs_title.append(\"--\")\n",
    "        \n",
    "    companies_tag=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]') # for companies names \n",
    "    for i in companies_tag:\n",
    "        try:\n",
    "            companies_name.append(i.text)\n",
    "        except NoSuchElementException   as e:\n",
    "            companies_name.append(\"--\")\n",
    "        \n",
    "    locations_tag=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span[1]') # for location names\n",
    "    for i in locations_tag:\n",
    "        try:\n",
    "            job_location.append(i.text)\n",
    "        except NoSuchElementException   as e:\n",
    "            job_location.append(\"--\")\n",
    "        \n",
    "    expriences_tag=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span[1]') # for year of expriences\n",
    "    for i in expriences_tag:\n",
    "        try:\n",
    "            expriences.append(i.text)\n",
    "        except NoSuchElementException   as e:\n",
    "            expriences.append(\"--\")\n",
    "    \n",
    "    Skills_tag=driver.find_elements_by_xpath('//ul[@class=\"tags has-description\"]') # for Skills\n",
    "    for i in Skills_tag:\n",
    "        try:\n",
    "            Skills.append(i.text.replace(\"\\n\",\",\").replace(\"IT Skills,\",\"\"))\n",
    "        except NoSuchElementException   as e:\n",
    "            Skills.append(\"--\")\n",
    "    driver.find_element_by_xpath(\"//div[@class='pagination mt-64 mb-60']/a[2]\").click()\n",
    "    time.sleep(4)\n",
    "\n",
    "## print the length of all the lists\n",
    "print(len(jobs_title),len(companies_name),len(job_location),len(expriences),len(Skills))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the dataframe \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Naukri_jobs = pd.DataFrame({})\n",
    "Naukri_jobs[\"jobs_title\"]=jobs_title\n",
    "Naukri_jobs[\"companies_name\"]=companies_name\n",
    "Naukri_jobs[\"locations_name\"]=job_location\n",
    "Naukri_jobs[\"expriences_year\"]=expriences\n",
    "Naukri_jobs[\"Skills\"]=Skills[0:100]\n",
    "\n",
    "Naukri_jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION- 8\n",
    "\n",
    "### Scrape the details of Highest selling novels.\n",
    "#### Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\n",
    "#### You have to find the following details:\n",
    "* A) Book name\n",
    "* B) Author name\n",
    "* C) Volumes sold\n",
    "* D) Publisher\n",
    "* E) Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the basic Libraries import requests\n",
    "\n",
    "import time\n",
    "import selenium\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "## lets connect with web driver \n",
    "\n",
    "#driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "## spacifiying the url\n",
    "Url=  \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "## lets open the web page through our web page \n",
    "#driver.get(Url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the BeautifulSoup for extect the Table\n",
    "\n",
    "page = requests.get(Url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "novels_tables = soup.find_all(\"table\")\n",
    "\n",
    "## scrap the tabuler data and find following details A) Name B) Description\n",
    "novels=novels_tables[0]\n",
    "novels_table_data = [[cell.text for cell in row.find_all([\"th\",\"td\"])]\n",
    "                               for row in novels.find_all(\"tr\")]\n",
    "\n",
    "## creating the dataframe \n",
    "novels_table_data_df = pd.DataFrame(novels_table_data,columns=[\"Rank\",\"Title\",\"Author\", \"Volume Sales\",\"Publisher\",\"Genre\"])\n",
    "novels_table_data_df.drop([0, 1],inplace=True)\n",
    "novels_table_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question:-9\n",
    "\n",
    "### Scrape the details most watched tv series of all time from imdb.com. Url = https://www.imdb.com/list/ls095964455/\n",
    "### You have to find the following details:\n",
    "* A) Name\n",
    "* B) Year span\n",
    "* C) Genre\n",
    "* D) Run time\n",
    "* E) Ratings\n",
    "* F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the basic Libraries import requests\n",
    "\n",
    "import time\n",
    "import selenium\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "## spacifiying the url\n",
    "Url=  \"https://www.imdb.com/list/ls095964455/\"\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the Empty folder\n",
    "Show_Name = []\n",
    "Year_span = []\n",
    "Genre = []\n",
    "Run_time = []\n",
    "Ratings = []\n",
    "Votes = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show_Name\n",
    "\n",
    "Show_Name_tag=driver.find_elements_by_xpath('//h3[@class=\"lister-item-header\"]/a') # for Show_Name\n",
    "for i in Show_Name_tag:\n",
    "    try:\n",
    "        Show_Name.append(i.text)\n",
    "    except NoSuchElementException   as e:\n",
    "        Show_Name.append(\"--\")\n",
    "            \n",
    "Show_Name[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Year_span\n",
    "\n",
    "Year_span_tag=driver.find_elements_by_xpath('//span[@class=\"lister-item-year text-muted unbold\"]') # for Year_span\n",
    "for i in Year_span_tag:\n",
    "    try:\n",
    "        Year_span.append(i.text)\n",
    "    except NoSuchElementException   as e:\n",
    "        Year_span.append(\"--\")\n",
    "Year_span[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Genre\n",
    "\n",
    "Genre_tag = driver.find_elements_by_xpath('//span[@class=\"genre\"]')\n",
    "for i in Genre_tag:\n",
    "    try:\n",
    "        Genre.append(i.text)\n",
    "    except NoSuchElementException   as e:\n",
    "        Genre.append(\"--\")\n",
    "Genre[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run_time\n",
    "\n",
    "Run_time_tag = driver.find_elements_by_xpath('//span[@class=\"runtime\"]')\n",
    "for i in Run_time_tag:\n",
    "    try:\n",
    "        Run_time.append(i.text)\n",
    "    except NoSuchElementException   as e:\n",
    "        Run_time.append(\"--\")\n",
    "Run_time[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Ratings\n",
    "Ratings_tag = driver.find_elements_by_xpath('//div[@class=\"ipl-rating-star small\"]/span[2]')\n",
    "for i in Ratings_tag:\n",
    "    try:\n",
    "        Ratings.append(i.text)\n",
    "    except NoSuchElementException   as e:\n",
    "        Ratings.append(\"--\")\n",
    "Ratings[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Votes\n",
    "Votes = []\n",
    "\n",
    "Votes_tag = driver.find_elements_by_xpath('//p[@class=\"text-muted text-small\"]/span[2]')\n",
    "for i in Votes_tag:\n",
    "    try:\n",
    "        Votes.append(i.text)\n",
    "    except NoSuchElementException   as e:\n",
    "        Votes.append(\"--\")\n",
    "print(Votes[0:5])\n",
    "\n",
    "Final_votes = []### fiter only \n",
    "for i in range(0, len(Votes)): \n",
    "    if i % 2: \n",
    "        Final_votes.append(Votes[i])\n",
    "print(Final_votes[0:5])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating the dataframe :\n",
    "\n",
    "IMDB_TV_df = pd.DataFrame({})\n",
    "\n",
    "IMDB_TV_df[\"Show_Name\"]= Show_Name\n",
    "IMDB_TV_df[\"Year span\"] = Year_span\n",
    "IMDB_TV_df[\"Genre\"]= Genre\n",
    "IMDB_TV_df[\"Run time\"] = Run_time\n",
    "IMDB_TV_df[\"Ratings\"] = Ratings\n",
    "IMDB_TV_df[\"Votes\"] = Final_votes\n",
    "\n",
    "IMDB_TV_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-10\n",
    "### Details of Datasets from UCI machine learning repositories. Url = https://archive.ics.uci.edu/\n",
    "### You have to find the following details:\n",
    "* A) Dataset name\n",
    "* B) Data type\n",
    "* C) Task\n",
    "* D) Attribute type\n",
    "* E) No of instances\n",
    "* F) No of attribute\n",
    "* G) Year\n",
    "## Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the basic Libraries import requests\n",
    "\n",
    "import time\n",
    "import selenium\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "## spacifiying the url\n",
    "Url=  \"https://archive.ics.uci.edu/ml/index.php\"\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "## click on the view all the datasets\n",
    "time.sleep(3)\n",
    "driver.find_element_by_xpath('/html/body/table[1]/tbody/tr/td[2]/span[2]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Link = driver.current_url\n",
    "page = requests.get(Link)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "UCI_tables = soup.find_all(\"table\")\n",
    "\n",
    "## scrap the tabuler data and find following details A) Name B) Description\n",
    "UCI=UCI_tables[5]\n",
    "UCI_table_data = [[cell.text for cell in row.find_all([\"th\",\"td\"])]\n",
    "                               for row in UCI.find_all(\"tr\")]\n",
    "\n",
    "## creating the dataframe \n",
    "UCI_table_data_df = pd.DataFrame(UCI_table_data Types\")\n",
    "UCI_table_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "357px",
    "left": "996px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
